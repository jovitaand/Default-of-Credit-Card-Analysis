{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN32YBAROaaV0PRSifae+gq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jovitaand/Default-of-Credit-Card-Clients/blob/main/Default_Of_Credit_Card_Clients.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "5Liqeg_yyqR7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_1dOQXLyU0S"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "import gc\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "\n",
        "pd.set_option('display.max_columns', 100)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/default of credit card clients.xlsx'\n",
        "data = pd.ExcelFile(file_path)\n",
        "#checking if there are any other excel sheets\n",
        "sheet_names = data.sheet_names\n",
        "print(sheet_names)"
      ],
      "metadata": {
        "id": "R6XomDOsy7zT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.read_excel(\"/content/default of credit card clients.xlsx\")\n",
        "data.head()"
      ],
      "metadata": {
        "id": "qjy_7lugzifb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dealing with the null values\n",
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "3X2As3iR0ATV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info() #looks we would need to delete the first row and shift the content up"
      ],
      "metadata": {
        "id": "OYEnIBiG0kQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reassigning proper column names manually from the observed structure\n",
        "data.columns = [\n",
        "    \"ID\", \"LIMIT_BAL\", \"SEX\", \"EDUCATION\", \"MARRIAGE\", \"AGE\",\n",
        "    \"PAY_0\", \"PAY_2\", \"PAY_3\", \"PAY_4\", \"PAY_5\", \"PAY_6\",\n",
        "    \"BILL_AMT1\", \"BILL_AMT2\", \"BILL_AMT3\", \"BILL_AMT4\", \"BILL_AMT5\", \"BILL_AMT6\",\n",
        "    \"PAY_AMT1\", \"PAY_AMT2\", \"PAY_AMT3\", \"PAY_AMT4\", \"PAY_AMT5\", \"PAY_AMT6\",\n",
        "    \"default.payment.next.month\"\n",
        "]\n",
        "\n",
        "# Remove any additional non-numeric or header rows\n",
        "data = data[1:]  # Exclude the duplicated header row\n",
        "data.reset_index(drop=True, inplace=True)  # Reset index\n",
        "\n",
        "# Convert data types for numeric columns\n",
        "for col in data.columns[1:]:\n",
        "    data[col] = pd.to_numeric(data[col], errors='coerce')\n",
        "\n",
        "# Display the first few rows to confirm the structure\n",
        "data\n"
      ],
      "metadata": {
        "id": "5RUk_maA1LMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "h3ukqtuM1ns_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe()"
      ],
      "metadata": {
        "id": "ateQU2WI1sDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering\n",
        "\n",
        "The training and testing datasets have been enhanced with the follwoing new features:\n",
        "1. `CREDIT_UTILIZATION`: The total bill amount as a proportion of the credit limit.\n",
        "2. `AVG_REPAY_DELAY`: The average repayment delay across the last six months\n",
        "3. `MAX_REPAY_DELAY`: The maximum repayment delay in the last six months.\n",
        "4. `TOTAL_PAYMENTS`: The total payment amount over the last six months.\n",
        "5. `TOTAL_BILLS`: The total bill amount over the last six months.\n",
        "6. `PAYMENT_TO_BILL_RATIO`: The ratio of total payments to total bills."
      ],
      "metadata": {
        "id": "_4O4yi_i2dZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Engineering: Adding meaningful features to the training and testing datasets\n",
        "\n",
        "def add_features(data):\n",
        "    # Credit Utilization Rate\n",
        "    data['CREDIT_UTILIZATION'] = (data[['BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3',\n",
        "                                        'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6']].sum(axis=1)) / data['LIMIT_BAL']\n",
        "\n",
        "    # Average and Maximum Repayment Delay\n",
        "    data['AVG_REPAY_DELAY'] = data[['PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']].mean(axis=1)\n",
        "    data['MAX_REPAY_DELAY'] = data[['PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']].max(axis=1)\n",
        "\n",
        "    # Total Payments and Total Bills\n",
        "    data['TOTAL_PAYMENTS'] = data[['PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']].sum(axis=1)\n",
        "    data['TOTAL_BILLS'] = data[['BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6']].sum(axis=1)\n",
        "\n",
        "    # Payment-to-Bill Ratio\n",
        "    data['PAYMENT_TO_BILL_RATIO'] = data['TOTAL_PAYMENTS'] / data['TOTAL_BILLS']\n",
        "    data['PAYMENT_TO_BILL_RATIO'] = data['PAYMENT_TO_BILL_RATIO'].fillna(0)  # Handle division by zero\n",
        "\n",
        "    return data\n",
        "\n",
        "# Apply feature engineering to both training and testing datasets\n",
        "X_train_balanced = add_features(X_train_balanced)\n",
        "X_test = add_features(X_test)\n",
        "\n",
        "# Display the first few rows of the enhanced training data\n",
        "X_train_balanced.head()\n"
      ],
      "metadata": {
        "id": "gCdAQgtK2aGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to sample the data by retaining the important features such as\n",
        "1. `LIMIT_BAL` - Credit Limit\n",
        "2. `SEX`- Gender\n",
        "3. `EDUCATION` - Educational Level\n",
        "4. `MARRIAGE` - Marital Status\n",
        "5. `AGE` - Age of the individual\n",
        "6. `CREDIT_UTILIZATION` - Credit utilization rate\n",
        "7. `AVG_REPAY_DELAY` - Average repayment delay\n",
        "8. `MAX_REPAY_DELAY` - maximum repayment delay\n",
        "10. `TOTAL_PAYMENTS` - Total payment amounts\n",
        "11. `TOTAL_BILLS` - Total payment amounts\n",
        "12. `PAYMENT_TO_BILL_RATIO` - Total billed amounts"
      ],
      "metadata": {
        "id": "64LH6wfG3OYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retain only the specified features in a new DataFrame called data1\n",
        "selected_features = [\n",
        "    \"LIMIT_BAL\", \"SEX\", \"EDUCATION\", \"MARRIAGE\", \"AGE\",\n",
        "    \"CREDIT_UTILIZATION\", \"AVG_REPAY_DELAY\", \"MAX_REPAY_DELAY\",\n",
        "    \"TOTAL_PAYMENTS\", \"TOTAL_BILLS\", \"PAYMENT_TO_BILL_RATIO\"\n",
        "]\n",
        "\n",
        "# Create data1 with the selected features\n",
        "data1 = X_train_balanced[selected_features]\n",
        "\n",
        "# Display the first few rows of the new downsized dataset\n",
        "data1.head()"
      ],
      "metadata": {
        "id": "gDHudX9K3-r1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Feature Analysis: Statistical summary\n",
        "feature_stats = data1.describe()\n",
        "\n",
        "# Correlation Analysis: Compute pairwise correlations\n",
        "correlation_matrix = data1.corr()\n",
        "\n",
        "# Visualize the correlation matrix using a heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar=True)\n",
        "plt.title(\"Feature Correlation Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "# Visualize distributions of key features\n",
        "for feature in data1.columns:\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    sns.histplot(data1[feature], kde=True, bins=30, color=\"blue\", edgecolor=\"black\")\n",
        "    plt.title(f\"Distribution of {feature}\")\n",
        "    plt.xlabel(feature)\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.grid(axis=\"y\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "W0YpRLT_45Xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpretation of Visualizations\n",
        "\n",
        "#### 1. Feature Correlation Heatmap\n",
        "**High Correlations:**\n",
        "- **CREDIT_UTILIZATION and TOTAL_BILLS**: These features are strongly correlated, as higher credit utilization is often driven by higher total bills.\n",
        "- **TOTAL_BILLS and LIMIT_BAL**: Individuals with higher credit limits tend to have higher total bills.\n",
        "\n",
        "**Weak Correlations:**\n",
        "- **AGE**, **MARRIAGE**, **SEX**, and most demographic features show weak correlations with numeric features like **CREDIT_UTILIZATION**, suggesting less direct influence on financial behavior.\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. Feature Distributions\n",
        "- **LIMIT_BAL (Credit Limit):**\n",
        "  - The distribution is right-skewed, with most values clustering below 300,000.\n",
        "  - Few individuals have exceptionally high limits, creating outliers.\n",
        "\n",
        "- **AGE:**\n",
        "  - A fairly normal distribution, peaking around 30–40 years.\n",
        "  - Few data points for ages above 60.\n",
        "\n",
        "- **CREDIT_UTILIZATION:**\n",
        "  - A right-skewed distribution, with a peak near lower utilization rates.\n",
        "  - A small number of extreme outliers suggest heavy over-utilization.\n",
        "\n",
        "- **AVG_REPAY_DELAY and MAX_REPAY_DELAY:**\n",
        "  - Peaks near 0 indicate that most individuals either repay on time or experience minor delays.\n",
        "  - A smaller portion of extreme delays shows heightened risk behaviors.\n",
        "\n",
        "- **TOTAL_PAYMENTS and TOTAL_BILLS:**\n",
        "  - Both are heavily right-skewed, with the majority of values concentrated at lower ranges.\n",
        "  - Extreme values likely indicate higher-income individuals or atypical billing patterns.\n",
        "\n",
        "- **PAYMENT_TO_BILL_RATIO:**\n",
        "  - Distribution indicates most individuals pay a small fraction of their bills regularly.\n",
        "  - Negative or extreme high ratios likely result from specific billing anomalies or refunds.\n"
      ],
      "metadata": {
        "id": "L50qEJbp6YAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Display the statistical summary as plain output\n",
        "feature_stats"
      ],
      "metadata": {
        "id": "_MgPR05f54RN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This summary highlights:\n",
        "\n",
        "* Wide ranges in `TOTAL_PAYMENTS` and `TOTAL_BILLS`.\n",
        "* Potential outliers in features like `CREDIT_UTILIZATION`, `PAYMENT_TO_BILL_RATIO`, and `TOTAL_BILLS` with extreme values.\n",
        "* Some rows may require further review or capping for outliers."
      ],
      "metadata": {
        "id": "c2IWS-_V6Did"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Next Steps\n",
        "1. Handle Outliers:\n",
        "\n",
        "* Address extreme values in key features like `CREDIT_UTILIZATION`, `TOTAL_BILLS`, and `PAYMENT_TO_BILL_RATIO`.\n",
        "\n",
        "2. Feature Scaling:\n",
        "\n",
        "* Apply scaling (e.g., MinMaxScaler or StandardScaler) to normalize the feature ranges, ensuring all features contribute proportionally to model training.\n",
        "\n",
        "3. Model Training:\n",
        "\n",
        "* Train a predictive model (e.g., logistic regression, random forest) using the downsized dataset.\n",
        "Evaluate using metrics like precision, recall, and balanced accuracy.\n",
        "\n",
        "4. Fairness and Explainability:\n",
        "\n",
        "* Use Fairlearn for fairness analysis and LIME/SHAP to interpret feature importance."
      ],
      "metadata": {
        "id": "PgbGDZak6yVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the entire dataset to numeric where possible\n",
        "data1_numeric = data1.apply(pd.to_numeric, errors='coerce')\n",
        "# Check for problematic values (NaN or infinite) in the dataset\n",
        "problematic_summary = {\n",
        "    \"NaN_Count\": data1_numeric.isna().sum(),\n",
        "    \"Inf_Count\": (data1_numeric == float('inf')).sum() + (data1_numeric == -float('inf')).sum(),\n",
        "}\n",
        "\n",
        "problematic_summary_df = pd.DataFrame(problematic_summary)\n",
        "problematic_summary_df\n"
      ],
      "metadata": {
        "id": "wIUjkmAF6o3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problematic Values Identified:\n",
        "1. The `PAYMENT_TO_BILL_RATIO` column contains 91 infinite values.\n",
        "2. No NaN values were detected in the dataset"
      ],
      "metadata": {
        "id": "rNOlrCce9N5U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution:\n",
        "* Replace infinite values in `PAYMENT_TO_BILL_RATIO` with a reasonable approximation, such as the column's maximum finite value."
      ],
      "metadata": {
        "id": "vyAgV4xs9wNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the RobustScaler again\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "# Redefine the RobustScaler instance\n",
        "scaler = RobustScaler()\n",
        "\n",
        "# Reapply scaling using RobustScaler\n",
        "scaled_data = pd.DataFrame(scaler.fit_transform(data1_numeric), columns=data1_numeric.columns)\n",
        "\n",
        "# Display the first few rows of the scaled dataset\n",
        "scaled_data.head()\n"
      ],
      "metadata": {
        "id": "kNhKV1_59YAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the features are now normalized!"
      ],
      "metadata": {
        "id": "rPc9vWVw-rCV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training\n",
        "\n",
        "Let’s begin by training a basic predictive model on the scaled data. Here's the plan:\n",
        "\n",
        "Split the Dataset:\n",
        "\n",
        "Separate features (X) and target (y) using the preprocessed data.\n",
        "Use the Default column as the target variable.\n",
        "Train a Logistic Regression Model:\n",
        "\n",
        "Train the model and evaluate its performance.\n",
        "Evaluate Performance:\n",
        "\n",
        "Calculate metrics like accuracy, precision, recall, and F1-score."
      ],
      "metadata": {
        "id": "p-OhTLIh-6fl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix, classification_report, balanced_accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Ensure correct target column reference\n",
        "X = data.drop(columns=[\"default.payment.next.month\", \"data_ID\"], errors=\"ignore\")  # Drop ID and irrelevant columns\n",
        "y = data[\"default.payment.next.month\"]\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Combine training features and target for resampling\n",
        "train_data = X_train.copy()\n",
        "train_data['default.payment.next.month'] = y_train\n",
        "\n",
        "# Separate majority and minority classes\n",
        "majority_class = train_data[train_data['default.payment.next.month'] == 0]\n",
        "minority_class = train_data[train_data['default.payment.next.month'] == 1]\n",
        "\n",
        "# Oversample the minority class\n",
        "minority_oversampled = resample(\n",
        "    minority_class,\n",
        "    replace=True,  # Sample with replacement\n",
        "    n_samples=len(majority_class),  # Match the number of majority class samples\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Combine oversampled minority class with majority class\n",
        "balanced_train_data = pd.concat([majority_class, minority_oversampled])\n",
        "\n",
        "# Separate features and target\n",
        "X_train_balanced = balanced_train_data.drop(columns=['default.payment.next.month'])\n",
        "y_train_balanced = balanced_train_data['default.payment.next.month']\n",
        "\n",
        "# Check the distribution of the target variable before and after oversampling\n",
        "before_balance = y_train.value_counts()\n",
        "after_balance = y_train_balanced.value_counts()\n",
        "\n",
        "print(\"Distribution before balancing:\")\n",
        "print(before_balance)\n",
        "print(\"\\nDistribution after balancing:\")\n",
        "print(after_balance)"
      ],
      "metadata": {
        "id": "Jquuv3z8b1Qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure numeric data for XGBoost\n",
        "X_train_balanced_numeric = X_train_balanced.select_dtypes(include=[\"number\"])\n",
        "X_test_numeric = X_test.select_dtypes(include=[\"number\"])\n",
        "\n",
        "# Train a Random Forest Classifier on the balanced dataset\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "rf_model.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
        "classification_report_summary = classification_report(y_test, y_pred)\n",
        "\n",
        "# Display the results\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(\"\\nBalanced Accuracy:\", f\"{balanced_acc:.2%}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report_summary)"
      ],
      "metadata": {
        "id": "hJh7RFAPb-ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train an XGBoost model\n",
        "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "xgb_model.fit(X_train_balanced_numeric, y_train_balanced)\n",
        "\n",
        "# Predict on the test set using XGBoost\n",
        "y_pred_xgb = xgb_model.predict(X_test_numeric)\n",
        "\n",
        "# Evaluate XGBoost model performance\n",
        "conf_matrix_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
        "balanced_acc_xgb = balanced_accuracy_score(y_test, y_pred_xgb)\n",
        "classification_report_xgb = classification_report(y_test, y_pred_xgb)\n",
        "\n",
        "print(\"\\nXGBoost Confusion Matrix:\")\n",
        "print(conf_matrix_xgb)\n",
        "print(\"\\nXGBoost Balanced Accuracy:\", f\"{balanced_acc_xgb:.2%}\")\n",
        "print(\"\\nXGBoost Classification Report:\")\n",
        "print(classification_report_xgb)"
      ],
      "metadata": {
        "id": "hi04oGZhcBM3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}